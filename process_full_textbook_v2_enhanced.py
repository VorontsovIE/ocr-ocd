#!/usr/bin/env python3
"""
OCR-OCD v2 Enhanced: GPT-4 Vision + EasyOCR Integration
=====================================================

–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è: GPT-4 Vision + EasyOCR –¥–∞–Ω–Ω—ã–µ
"""

import sys
import json
import time
import base64
import click
import openai
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Any, Optional
from dotenv import load_dotenv

# –î–æ–±–∞–≤–ª—è–µ–º src –≤ Python path
sys.path.insert(0, str(Path(__file__).parent / "src"))

from src.core.pdf_processor import PDFProcessor
from src.core.data_extractor import DataExtractor
from src.core.csv_writer import CSVWriter
from src.utils.logger import setup_development_logger, setup_production_logger, get_logger
from src.utils.state_manager import StateManager
from src.utils.easyocr_parser import EasyOCRParser


class EnhancedVisionAPI:
    """–£–ª—É—á—à–µ–Ω–Ω—ã–π –∫–ª–∏–µ–Ω—Ç –¥–ª—è GPT-4 Vision API —Å –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–µ–π EasyOCR."""
    
    def __init__(self, ocr_file_path: Optional[str] = None):
        load_dotenv()
        self.client = openai.OpenAI()
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è EasyOCR –ø–∞—Ä—Å–µ—Ä–∞ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        self.ocr_parser = None
        if ocr_file_path and Path(ocr_file_path).exists():
            try:
                self.ocr_parser = EasyOCRParser(ocr_file_path)
                print(f"‚úÖ EasyOCR –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –∞–∫—Ç–∏–≤–Ω–∞: {len(self.ocr_parser.get_available_pages())} —Å—Ç—Ä–∞–Ω–∏—Ü")
            except Exception as e:
                print(f"‚ö†Ô∏è  EasyOCR –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: {e}")
                self.ocr_parser = None
        else:
            print("üìã –†–∞–±–æ—Ç–∞–µ–º —Ç–æ–ª—å–∫–æ —Å GPT-4 Vision (–±–µ–∑ EasyOCR)")
        
    def extract_tasks_from_page(self, image_data: bytes, page_number: int) -> Dict[str, Any]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∑–∞–¥–∞—á–∏ —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏—Å–ø–æ–ª—å–∑—É—è GPT-4 Vision + EasyOCR."""
        
        # –ö–æ–¥–∏—Ä—É–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
        b64_image = base64.b64encode(image_data).decode('utf-8')
        
        # –ë–∞–∑–æ–≤—ã–π –ø—Ä–æ–º–ø—Ç
        base_prompt = f"""–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —Å—Ç—Ä–∞–Ω–∏—Ü—É {page_number} –∏–∑ —É—á–µ–±–Ω–∏–∫–∞ –º–∞—Ç–µ–º–∞—Ç–∏–∫–∏ –¥–ª—è 1 –∫–ª–∞—Å—Å–∞ (1959 –≥–æ–¥).

–ù–∞–π–¥–∏ –≤—Å–µ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –∑–∞–¥–∞—á–∏ –∏ —É–ø—Ä–∞–∂–Ω–µ–Ω–∏—è –Ω–∞ —ç—Ç–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü–µ.
–î–ª—è –∫–∞–∂–¥–æ–π –∑–∞–¥–∞—á–∏ –≤–µ—Ä–Ω–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –≤ JSON —Ñ–æ—Ä–º–∞—Ç–µ:

{{
  "page_number": {page_number},
  "tasks": [
    {{
      "task_number": "1",
      "task_text": "–ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç –∑–∞–¥–∞—á–∏",
      "has_image": true
    }}
  ]
}}

–í–ê–ñ–ù–û:
- –ï—Å–ª–∏ –Ω–æ–º–µ—Ä –∑–∞–¥–∞—á–∏ –Ω–µ –≤–∏–¥–Ω–æ, –∏—Å–ø–æ–ª—å–∑—É–π "unknown-1", "unknown-2" –∏ —Ç.–¥.
- –í task_text –≤–∫–ª—é—á–∞–π –í–ï–°–¨ —Ç–µ–∫—Å—Ç –∑–∞–¥–∞—á–∏ –≤–∫–ª—é—á–∞—è —á–∏—Å–ª–∞ –∏ —É—Å–ª–æ–≤–∏—è
- has_image = true –µ—Å–ª–∏ –µ—Å—Ç—å —Ä–∏—Å—É–Ω–∫–∏, —Å—Ö–µ–º—ã, –¥–∏–∞–≥—Ä–∞–º–º—ã –∫ –∑–∞–¥–∞—á–µ
- –ò–≥–Ω–æ—Ä–∏—Ä—É–π –Ω–æ–º–µ—Ä–∞ —Å—Ç—Ä–∞–Ω–∏—Ü, –∑–∞–≥–æ–ª–æ–≤–∫–∏, –Ω–∞–∑–≤–∞–Ω–∏–µ —É—á–µ–±–Ω–∏–∫–∞
- –û—Ç–≤–µ—Ç—å –¢–û–õ–¨–ö–û JSON, –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞"""

        # –î–æ–ø–æ–ª–Ω—è–µ–º –ø—Ä–æ–º–ø—Ç –¥–∞–Ω–Ω—ã–º–∏ EasyOCR –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω—ã
        ocr_supplement = ""
        if self.ocr_parser:
            ocr_supplement = self.ocr_parser.create_vision_prompt_supplement(page_number)
            if ocr_supplement:
                base_prompt += f"\n\n{ocr_supplement}"

        try:
            start_time = time.time()
            
            response = self.client.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {
                        "role": "user", 
                        "content": [
                            {"type": "text", "text": base_prompt},
                            {"type": "image_url", "image_url": {"url": f"data:image/png;base64,{b64_image}"}}
                        ]
                    }
                ],
                max_tokens=2500,  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –ª–∏–º–∏—Ç –¥–ª—è –±–æ–ª–µ–µ –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
                temperature=0.05   # –û—á–µ–Ω—å –Ω–∏–∑–∫–∞—è —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –¥–ª—è —Ç–æ—á–Ω–æ—Å—Ç–∏
            )
            
            processing_time = time.time() - start_time
            content = response.choices[0].message.content
            
            # –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –ø–∞—Ä—Å–∏–Ω–≥ JSON
            try:
                # –ü–æ–ø—ã—Ç–∫–∞ –ø—Ä—è–º–æ–≥–æ –ø–∞—Ä—Å–∏–Ω–≥–∞
                parsed_data = json.loads(content)
                json_valid = True
            except json.JSONDecodeError:
                # –ü–æ–ø—ã—Ç–∫–∞ –∏–∑–≤–ª–µ—á—å JSON –∏–∑ —Ç–µ–∫—Å—Ç–∞
                json_valid = False
                try:
                    # –ò—â–µ–º JSON –±–ª–æ–∫ –≤ –æ—Ç–≤–µ—Ç–µ
                    import re
                    json_match = re.search(r'\{.*\}', content, re.DOTALL)
                    if json_match:
                        parsed_data = json.loads(json_match.group())
                        json_valid = True
                    else:
                        raise ValueError("No JSON found")
                except:
                    # –ï—Å–ª–∏ –≤—Å—ë –ø–ª–æ—Ö–æ, —Å–æ–∑–¥–∞—ë–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É –≤—Ä—É—á–Ω—É—é –∏–∑ —Ç–µ–∫—Å—Ç–∞
                    parsed_data = self._parse_fallback_response(content, page_number)
            
            # –û–±–æ–≥–∞—â–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö OCR –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π
            if self.ocr_parser and json_valid:
                parsed_data = self._enrich_with_ocr_data(parsed_data, page_number)
            
            return {
                "content": content,
                "parsed_data": parsed_data,
                "json_valid": json_valid,
                "usage": response.usage.model_dump() if response.usage else {},
                "model": response.model,
                "processing_time": processing_time,
                "image_info": {"size_bytes": len(image_data)},
                "prompt_type": "enhanced_vision_ocr" if self.ocr_parser else "vision_only",
                "ocr_supplement_length": len(ocr_supplement) if ocr_supplement else 0
            }
            
        except Exception as e:
            print(f"‚ùå API –æ—à–∏–±–∫–∞ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {page_number}: {e}")
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º fallback —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Å OCR –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–æ
            fallback_data = self._create_ocr_fallback(page_number) if self.ocr_parser else {
                "page_number": page_number, 
                "tasks": []
            }
            
            return {
                "content": f"–û—à–∏–±–∫–∞ API: {e}",
                "parsed_data": fallback_data,
                "json_valid": False,
                "usage": {},
                "model": "error",
                "processing_time": 0,
                "image_info": {"size_bytes": len(image_data)},
                "prompt_type": "error_fallback"
            }
    
    def _parse_fallback_response(self, content: str, page_number: int) -> Dict[str, Any]:
        """–ü–∞—Ä—Å–∏—Ç –æ—Ç–≤–µ—Ç –µ—Å–ª–∏ JSON –Ω–µ —É–¥–∞–ª—Å—è."""
        lines = content.split('\n')
        tasks = []
        
        current_task = None
        task_counter = 1
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
                
            # –ü—Ä–æ—Å—Ç–∞—è —ç–≤—Ä–∏—Å—Ç–∏–∫–∞ –¥–ª—è –ø–æ–∏—Å–∫–∞ –∑–∞–¥–∞—á
            if any(keyword in line.lower() for keyword in ['–∑–∞–¥–∞—á', '–ø—Ä–∏–º–µ—Ä', '—Ä–µ—à–∏', '–Ω–∞–π–¥–∏', '—Å–∫–æ–ª—å–∫–æ']):
                if current_task:
                    tasks.append(current_task)
                current_task = {
                    "task_number": f"unknown-{task_counter}",
                    "task_text": line,
                    "has_image": "—Ä–∏—Å—É–Ω" in line.lower() or "—Å—Ö–µ–º" in line.lower()
                }
                task_counter += 1
            elif current_task and len(line) > 10:
                # –ü—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∑–∞–¥–∞—á–∏
                current_task["task_text"] += " " + line
        
        if current_task:
            tasks.append(current_task)
        
        return {
            "page_number": page_number,
            "tasks": tasks if tasks else [
                {
                    "task_number": "unknown-1",
                    "task_text": content[:200] + "..." if len(content) > 200 else content,
                    "has_image": False
                }
            ]
        }
    
    def _enrich_with_ocr_data(self, parsed_data: Dict[str, Any], page_number: int) -> Dict[str, Any]:
        """–û–±–æ–≥–∞—â–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ GPT-4 —Å –ø–æ–º–æ—â—å—é OCR –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏."""
        if not self.ocr_parser:
            return parsed_data
        
        try:
            ocr_page = self.ocr_parser.parse_page(page_number)
            if not ocr_page:
                return parsed_data
            
            # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ OCR
            if "page_info" not in parsed_data:
                parsed_data["page_info"] = {}
            
            parsed_data["page_info"].update({
                "ocr_word_count": ocr_page.word_count,
                "ocr_confidence": round(ocr_page.avg_confidence, 1),
                "ocr_numbers": ocr_page.get_numbers_and_operators()[:10],
                "ocr_integration": "enhanced"
            })
            
            # –£–ª—É—á—à–∞–µ–º task_text –µ—Å–ª–∏ –µ—Å—Ç—å —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è —Å OCR
            ocr_text_lower = ocr_page.text.lower()
            for task in parsed_data.get("tasks", []):
                task_text = task.get("task_text", "")
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –µ—Å—Ç—å –ª–∏ —á–∏—Å–ª–∞ –∏–∑ OCR –≤ —Ç–µ–∫—Å—Ç–µ –∑–∞–¥–∞—á–∏
                ocr_numbers = ocr_page.get_numbers_and_operators()
                task_numbers = [char for char in task_text if char.isdigit()]
                
                if ocr_numbers and not task_numbers:
                    # –ï—Å–ª–∏ –≤ –∑–∞–¥–∞—á–µ –Ω–µ—Ç —á–∏—Å–µ–ª, –Ω–æ –≤ OCR –µ—Å—Ç—å, –¥–æ–±–∞–≤–ª—è–µ–º –ø—Ä–∏–º–µ—á–∞–Ω–∏–µ
                    task["task_text"] += f" [–ß–∏—Å–ª–∞ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ: {', '.join(ocr_numbers[:5])}]"
            
            return parsed_data
            
        except Exception as e:
            print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –æ–±–æ–≥–∞—â–µ–Ω–∏—è OCR –¥–ª—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_number}: {e}")
            return parsed_data
    
    def _create_ocr_fallback(self, page_number: int) -> Dict[str, Any]:
        """–°–æ–∑–¥–∞—ë—Ç fallback —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∏—Å–ø–æ–ª—å–∑—É—è —Ç–æ–ª—å–∫–æ OCR –¥–∞–Ω–Ω—ã–µ."""
        if not self.ocr_parser:
            return {"page_number": page_number, "tasks": []}
        
        try:
            ocr_page = self.ocr_parser.parse_page(page_number)
            if not ocr_page:
                return {"page_number": page_number, "tasks": []}
            
            # –°–æ–∑–¥–∞—ë–º –ø—Ä–æ—Å—Ç—É—é –∑–∞–¥–∞—á—É –∏–∑ OCR –¥–∞–Ω–Ω—ã—Ö
            ocr_text = ocr_page.get_high_confidence_text()[:200]
            numbers = ocr_page.get_numbers_and_operators()
            
            return {
                "page_number": page_number,
                "tasks": [
                    {
                        "task_number": "ocr-fallback-1",
                        "task_text": f"OCR –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ: {ocr_text}",
                        "has_image": len(numbers) > 0,
                        "fallback_source": "easyocr"
                    }
                ]
            }
            
        except Exception as e:
            return {"page_number": page_number, "tasks": []}


class IntermediateStorage:
    """–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–º —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤."""
    
    def __init__(self, storage_dir: Path):
        self.storage_dir = storage_dir
        self.storage_dir.mkdir(parents=True, exist_ok=True)
        
    def save_page_result(self, page_number: int, page_data: Dict[str, Any]) -> None:
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã."""
        file_path = self.storage_dir / f"page_{page_number:03d}.json"
        
        # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        page_data.update({
            'processed_at': datetime.now().isoformat(),
            'page_number': page_number,
            'storage_version': '2.2-enhanced'
        })
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º datetime –æ–±—ä–µ–∫—Ç—ã –≤ —Å—Ç—Ä–æ–∫–∏
        page_data = self._serialize_datetime(page_data)
        
        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(page_data, f, ensure_ascii=False, indent=2)
    
    def load_page_result(self, page_number: int) -> Optional[Dict[str, Any]]:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã."""
        file_path = self.storage_dir / f"page_{page_number:03d}.json"
        
        if not file_path.exists():
            return None
            
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_number}: {e}")
            return None
    
    def get_processed_pages(self) -> List[int]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –Ω–æ–º–µ—Ä–æ–≤ —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü."""
        processed = []
        
        for file_path in sorted(self.storage_dir.glob("page_*.json")):
            try:
                page_num = int(file_path.stem.split('_')[1])
                processed.append(page_num)
            except (ValueError, IndexError):
                continue
                
        return sorted(processed)
    
    def load_all_results(self) -> List[Dict[str, Any]]:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –≤—Å–µ —Å–æ—Ö—Ä–∞–Ω—ë–Ω–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã."""
        results = []
        
        for page_num in self.get_processed_pages():
            page_data = self.load_page_result(page_num)
            if page_data:
                results.append(page_data)
        
        return results
    
    def clear_storage(self) -> None:
        """–û—á–∏—â–∞–µ—Ç –≤—Å–µ –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ —Ñ–∞–π–ª—ã."""
        for file_path in self.storage_dir.glob("page_*.json"):
            file_path.unlink()
    
    def _serialize_datetime(self, obj):
        """–†–µ–∫—É—Ä—Å–∏–≤–Ω–æ –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç datetime –æ–±—ä–µ–∫—Ç—ã –≤ —Å—Ç—Ä–æ–∫–∏."""
        if isinstance(obj, datetime):
            return obj.isoformat()
        elif isinstance(obj, dict):
            return {key: self._serialize_datetime(value) for key, value in obj.items()}
        elif isinstance(obj, list):
            return [self._serialize_datetime(item) for item in obj]
        else:
            return obj


@click.command()
@click.argument('pdf_path', type=click.Path(exists=True))
@click.argument('output_csv', type=click.Path())
@click.option('--ocr-file', type=click.Path(), help='–ü—É—Ç—å –∫ —Ñ–∞–π–ª—É EasyOCR TSV –¥–∞–Ω–Ω—ã—Ö')
@click.option('--force', is_flag=True, help='–ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –ø–µ—Ä–µ–æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –≤—Å–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã')
@click.option('--start-page', type=int, default=1, help='–ù–∞—á–∞–ª—å–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏')
@click.option('--end-page', type=int, default=None, help='–ö–æ–Ω–µ—á–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏')
@click.option('--production', is_flag=True, help='Production —Ä–µ–∂–∏–º –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è')
@click.option('--verbose', is_flag=True, help='–ü–æ–¥—Ä–æ–±–Ω—ã–π –≤—ã–≤–æ–¥')
def process_textbook_enhanced(pdf_path, output_csv, ocr_file, force, start_page, end_page, production, verbose):
    """
    OCR-OCD v2 Enhanced: GPT-4 Vision + EasyOCR Integration.
    
    –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á.
    """
    
    print("üöÄüìö OCR-OCD v2 Enhanced: GPT-4 Vision + EasyOCR")
    print("=" * 60)
    print(f"üìÑ PDF: {pdf_path}")
    print(f"üìä –í—ã–≤–æ–¥: {output_csv}")
    print(f"üîç EasyOCR —Ñ–∞–π–ª: {ocr_file if ocr_file else '–ù–µ —É–∫–∞–∑–∞–Ω'}")
    print(f"üîÑ Force —Ä–µ–∂–∏–º: {'‚úÖ' if force else '‚ùå'}")
    print(f"ü§ñ Enhanced AI Pipeline: ‚úÖ")
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
    if production:
        setup_production_logger()
    else:
        setup_development_logger()
    
    logger = get_logger(__name__)
    
    # –°–æ–∑–¥–∞—ë–º –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
    paths = {
        'output': Path("output"),
        'temp': Path("temp"), 
        'logs': Path("logs"),
        'storage': Path("temp/processed_pages_enhanced")
    }
    
    for path in paths.values():
        path.mkdir(parents=True, exist_ok=True)
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω–æ–≥–æ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞
    storage = IntermediateStorage(paths['storage'])
    
    if force:
        print("üóëÔ∏è  Force —Ä–µ–∂–∏–º: –æ—á–∏—Å—Ç–∫–∞ –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤...")
        storage.clear_storage()
    
    start_time = datetime.now()
    
    try:
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è PDF –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞
        print(f"üìÑ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è PDF –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞...")
        pdf_processor = PDFProcessor(
            pdf_path=pdf_path,
            temp_dir=paths['temp'],
            dpi=200,  # –í—ã—Å–æ–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏
            image_format="PNG"
        )
        
        with pdf_processor:
            pdf_processor.load_pdf()
            total_pages = pdf_processor.get_page_count()
            
            if end_page is None:
                end_page = total_pages
            else:
                end_page = min(end_page, total_pages)
            
            print(f"‚úÖ PDF –∑–∞–≥—Ä—É–∂–µ–Ω: {total_pages} —Å—Ç—Ä–∞–Ω–∏—Ü")
            print(f"üéØ –û–±—Ä–∞–±–æ—Ç–∫–∞: —Å—Ç—Ä–∞–Ω–∏—Ü—ã {start_page}-{end_page}")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            processed_pages = set(storage.get_processed_pages())
            
            if processed_pages and not force:
                print(f"üìã –ù–∞–π–¥–µ–Ω–æ {len(processed_pages)} —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü")
                print(f"üìä –ü–æ—Å–ª–µ–¥–Ω—è—è: {max(processed_pages) if processed_pages else 'none'}")
            
            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
            data_extractor = DataExtractor()
            enhanced_api = EnhancedVisionAPI(ocr_file)
            
            print(f"ü§ñ Enhanced Vision API –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
            
            # –°—á—ë—Ç—á–∏–∫–∏
            processed_count = 0
            skipped_count = 0 
            error_count = 0
            total_tasks = 0
            total_tokens = 0
            
            print(f"\nüöÄ –ù–∞—á–∏–Ω–∞–µ–º enhanced –æ–±—Ä–∞–±–æ—Ç–∫—É...")
            
            for page_num in range(start_page, end_page + 1):
                page_start_time = time.time()
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω—É–∂–Ω–æ –ª–∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É
                if not force and page_num in processed_pages:
                    skipped_count += 1
                    if verbose:
                        print(f"‚è≠Ô∏è  –°—Ç—Ä–∞–Ω–∏—Ü–∞ {page_num}: –ø—Ä–æ–ø—É—â–µ–Ω–∞ (—É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–∞)")
                    continue
                
                try:
                    progress = (page_num - start_page + 1) / (end_page - start_page + 1) * 100
                    print(f"\nüìñ –°—Ç—Ä–∞–Ω–∏—Ü–∞ {page_num}/{end_page} ({progress:.1f}%)")
                    
                    # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
                    if verbose:
                        print(f"  üñºÔ∏è  –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ...")
                    image_data = pdf_processor.convert_page_to_image(page_num - 1, save_to_file=False)
                    
                    # –û–±—Ä–∞–±–æ—Ç–∫–∞ —á–µ—Ä–µ–∑ Enhanced API
                    if verbose:
                        print(f"  ü§ñ Enhanced GPT-4 Vision + OCR –∑–∞–ø—Ä–æ—Å...")
                    
                    api_response = enhanced_api.extract_tasks_from_page(image_data, page_num)
                    
                    if verbose:
                        print(f"  ‚úÖ API –æ—Ç–≤–µ—Ç –ø–æ–ª—É—á–µ–Ω ({api_response['model']})")
                        tokens_used = api_response['usage'].get('total_tokens', 0)
                        print(f"  üìä Tokens: {tokens_used}")
                        print(f"  üîç OCR –¥–æ–ø–æ–ª–Ω–µ–Ω–∏–µ: {api_response.get('ocr_supplement_length', 0)} —Å–∏–º–≤–æ–ª–æ–≤")
                        total_tokens += tokens_used
                    
                    # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
                    if verbose:
                        print(f"  üìã –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∑–∞–¥–∞—á...")
                    
                    image_info = {
                        "size_bytes": len(image_data),
                        "format": "PNG",
                        "page_number": page_num
                    }
                    
                    page = data_extractor.parse_api_response(
                        api_response['parsed_data'],
                        page_num,
                        page_start_time,
                        image_info
                    )
                    
                    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω–æ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
                    page_result = {
                        'page_number': page_num,
                        'tasks': [task.model_dump() for task in page.tasks],
                        'processing_time': time.time() - page_start_time,
                        'image_info': image_info,
                        'api_method': 'enhanced_gpt4_vision_ocr',
                        'task_count': len(page.tasks),
                        'api_response': api_response,
                        'tokens_used': api_response['usage'].get('total_tokens', 0)
                    }
                    
                    storage.save_page_result(page_num, page_result)
                    
                    processed_count += 1
                    total_tasks += len(page.tasks)
                    
                    processing_time = time.time() - page_start_time
                    print(f"     ‚úÖ –ó–∞–¥–∞—á: {len(page.tasks)}, –í—Ä–µ–º—è: {processing_time:.1f}s")
                    
                    if verbose:
                        for i, task in enumerate(page.tasks, 1):
                            print(f"       {i}. {task.task_number}: {task.task_text[:50]}...")
                    
                    # –£–º–Ω–∞—è –ø–∞—É–∑–∞ (–±–æ–ª—å—à–µ –¥–ª—è —Å–ª–æ–∂–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü)
                    pause = 2 if len(page.tasks) > 2 else 1
                    time.sleep(pause)
                    
                except Exception as e:
                    logger.error(f"Error processing page {page_num}: {e}")
                    print(f"     ‚ùå –û—à–∏–±–∫–∞ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {page_num}: {e}")
                    error_count += 1
                    continue
            
            # –§–∏–Ω–∞–ª—å–Ω—ã–π —Å–±–æ—Ä —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            print(f"\nüíæ –°–±–æ—Ä —Ñ–∏–Ω–∞–ª—å–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤...")
            all_results = storage.load_all_results()
            
            # –°–æ–∑–¥–∞–Ω–∏–µ –∏—Ç–æ–≥–æ–≤–æ–≥–æ CSV
            if all_results:
                print(f"üìä –°–æ–∑–¥–∞–Ω–∏–µ enhanced CSV —Ñ–∞–π–ª–∞...")
                create_enhanced_csv(all_results, output_csv)
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            total_time = (datetime.now() - start_time).total_seconds()
            
            print(f"\nüéâ ENHANCED –û–ë–†–ê–ë–û–¢–ö–ê –ó–ê–í–ï–†–®–ï–ù–ê!")
            print(f"=" * 50)
            print(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
            print(f"   üìö –°—Ç—Ä–∞–Ω–∏—Ü –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {processed_count}")
            print(f"   ‚è≠Ô∏è  –°—Ç—Ä–∞–Ω–∏—Ü –ø—Ä–æ–ø—É—â–µ–Ω–æ: {skipped_count}")
            print(f"   ‚ùå –û—à–∏–±–æ–∫: {error_count}")
            print(f"   üìù –ó–∞–¥–∞—á –∏–∑–≤–ª–µ—á–µ–Ω–æ: {total_tasks}")
            print(f"   üéØ –û–±—â–∏–µ —Ç–æ–∫–µ–Ω—ã: {total_tokens:,}")
            print(f"   üí∞ –ü—Ä–∏–º–µ—Ä–Ω–∞—è —Å—Ç–æ–∏–º–æ—Å—Ç—å: ${total_tokens * 0.01 / 1000:.2f}")
            print(f"   ‚è±Ô∏è  –û–±—â–µ–µ –≤—Ä–µ–º—è: {total_time:.1f}s ({total_time/60:.1f} –º–∏–Ω—É—Ç)")
            if processed_count > 0:
                print(f"   ‚ö° –°–∫–æ—Ä–æ—Å—Ç—å: {processed_count/total_time*60:.1f} —Å—Ç—Ä–∞–Ω–∏—Ü/–º–∏–Ω—É—Ç—É")
                print(f"   üìà –°—Ä–µ–¥–Ω–µ–µ –∑–∞–¥–∞—á/—Å—Ç—Ä–∞–Ω–∏—Ü–∞: {total_tasks/processed_count:.1f}")
            print(f"   üìÅ Enhanced CSV: {output_csv}")
            
            return True
            
    except Exception as e:
        logger.error(f"Critical error: {e}")
        print(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
        return False


def create_enhanced_csv(results: List[Dict], output_path: str) -> None:
    """–°–æ–∑–¥–∞—ë—Ç enhanced CSV —Ñ–∞–π–ª –∏–∑ –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤."""
    
    all_tasks = []
    
    for page_result in results:
        page_num = page_result['page_number']
        processing_time = page_result.get('processing_time', 0)
        api_method = page_result.get('api_method', 'unknown')
        tokens_used = page_result.get('tokens_used', 0)
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º OCR –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –µ—Å–ª–∏ –µ—Å—Ç—å
        api_response = page_result.get('api_response', {})
        ocr_supplement_length = api_response.get('ocr_supplement_length', 0)
        
        for task_data in page_result.get('tasks', []):
            task_row = {
                'page_number': page_num,
                'task_number': task_data.get('task_number', 'unknown'),
                'task_text': task_data.get('task_text', ''),
                'has_image': task_data.get('has_image', False),
                'confidence_score': task_data.get('confidence_score', 0.0),
                'processing_time': processing_time,
                'api_method': api_method,
                'tokens_used': tokens_used,
                'ocr_supplement_used': ocr_supplement_length > 0,
                'ocr_supplement_length': ocr_supplement_length,
                'extracted_at': page_result.get('processed_at', ''),
                'word_count': len(task_data.get('task_text', '').split()),
                'enhanced_features': 'gpt4_vision_easyocr_integration'
            }
            all_tasks.append(task_row)
    
    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –Ω–æ–º–µ—Ä—É —Å—Ç—Ä–∞–Ω–∏—Ü—ã
    all_tasks.sort(key=lambda x: x['page_number'])
    
    # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º CSV
    import csv
    with open(output_path, 'w', newline='', encoding='utf-8') as f:
        if all_tasks:
            fieldnames = list(all_tasks[0].keys())
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            writer.writeheader()
            writer.writerows(all_tasks)
    
    print(f"‚úÖ Enhanced CSV —Å–æ–∑–¥–∞–Ω: {len(all_tasks)} –∑–∞–¥–∞—á —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ")
    print(f"üìä –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–æ–ª—è: tokens_used, ocr_supplement_used, enhanced_features")


if __name__ == "__main__":
    process_textbook_enhanced() 