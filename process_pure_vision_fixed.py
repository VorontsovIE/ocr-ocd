#!/usr/bin/env python3
"""
OCR-OCD Pure Vision Fixed: –ü—Ä—è–º–∞—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è OpenAI
==================================================

–ò—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω–∞—è —á–∏—Å—Ç–∞—è —Å–∏—Å—Ç–µ–º–∞:
- üìñ PDF ‚Üí –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Å—Ç—Ä–∞–Ω–∏—Ü
- üñºÔ∏è –ü—Ä—è–º–æ–π GPT-4 Vision API ‚Üí –∞–Ω–∞–ª–∏–∑ –∑–∞–¥–∞—á  
- üìä CSV ‚Üí —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã

–° —Ä–∞–±–æ—á–µ–π –ø—Ä—è–º–æ–π –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–µ–π OpenAI!
"""

import sys
import json
import time
import base64
import click
import openai
import asyncio
import concurrent.futures
import hashlib
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Any, Optional
from dotenv import load_dotenv
from PIL import Image, ImageEnhance, ImageFilter
import io

# –î–æ–±–∞–≤–ª—è–µ–º src –≤ Python path
sys.path.insert(0, str(Path(__file__).parent / "src"))

from src.core.pdf_processor import PDFProcessor
from src.utils.logger import setup_development_logger, setup_production_logger, get_logger


def generate_file_identifier(pdf_path: str) -> str:
    """
    –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —É–Ω–∏–∫–∞–ª—å–Ω—ã–π –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä –¥–ª—è PDF —Ñ–∞–π–ª–∞.
    
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç MD5 —Ö–µ—à —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ —Ñ–∞–π–ª–∞ + basename –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è
    —É–Ω–∏–∫–∞–ª—å–Ω–æ–≥–æ –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–∑–≤–æ–ª—è–µ—Ç –∏–∑–±–µ–∂–∞—Ç—å 
    –∫–æ–Ω—Ñ–ª–∏–∫—Ç–æ–≤ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Ä–∞–∑–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤.
    
    Args:
        pdf_path: –ü—É—Ç—å –∫ PDF —Ñ–∞–π–ª—É
        
    Returns:
        –£–Ω–∏–∫–∞–ª—å–Ω—ã–π –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä –≤ —Ñ–æ—Ä–º–∞—Ç–µ "basename_md5hash"
    """
    file_path = Path(pdf_path)
    basename = file_path.stem  # –ò–º—è —Ñ–∞–π–ª–∞ –±–µ–∑ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è
    
    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º MD5 —Ö–µ—à —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ —Ñ–∞–π–ª–∞
    hash_md5 = hashlib.md5()
    with open(pdf_path, "rb") as f:
        # –ß–∏—Ç–∞–µ–º —Ñ–∞–π–ª –±–ª–æ–∫–∞–º–∏ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ —Å –±–æ–ª—å—à–∏–º–∏ —Ñ–∞–π–ª–∞–º–∏
        for chunk in iter(lambda: f.read(4096), b""):
            hash_md5.update(chunk)
    
    file_hash = hash_md5.hexdigest()[:8]  # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ 8 —Å–∏–º–≤–æ–ª–æ–≤ —Ö–µ—à–∞
    
    # –û—á–∏—â–∞–µ–º basename –æ—Ç –ø—Ä–æ–±–ª–µ–º–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤
    clean_basename = "".join(c for c in basename if c.isalnum() or c in "_-")[:20]
    
    return f"{clean_basename}_{file_hash}"


def split_image_for_analysis(image_data: bytes, split_mode: str = "vertical") -> List[bytes]:
    """
    –†–∞–∑–¥–µ–ª—è–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –Ω–∞ —á–∞—Å—Ç–∏ –¥–ª—è –ª—É—á—à–µ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –º–Ω–æ–≥–æ–∫–æ–ª–æ–Ω–æ—á–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü.
    
    Args:
        image_data: –î–∞–Ω–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        split_mode: –†–µ–∂–∏–º —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è ("vertical", "horizontal", "grid")
        
    Returns:
        –°–ø–∏—Å–æ–∫ —á–∞—Å—Ç–µ–π –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
    """
    try:
        image = Image.open(io.BytesIO(image_data))
        width, height = image.size
        parts = []
        
        if split_mode == "vertical":
            # –†–∞–∑–¥–µ–ª—è–µ–º –≤–µ—Ä—Ç–∏–∫–∞–ª—å–Ω–æ –Ω–∞ 2 —á–∞—Å—Ç–∏ (–ª–µ–≤–∞—è/–ø—Ä–∞–≤–∞—è)
            left_part = image.crop((0, 0, width // 2, height))
            right_part = image.crop((width // 2, 0, width, height))
            
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ bytes
            for part, name in [(left_part, "left"), (right_part, "right")]:
                buffer = io.BytesIO()
                part.save(buffer, format='PNG', quality=95)
                parts.append(buffer.getvalue())
                
        elif split_mode == "horizontal":
            # –†–∞–∑–¥–µ–ª—è–µ–º –≥–æ—Ä–∏–∑–æ–Ω—Ç–∞–ª—å–Ω–æ –Ω–∞ 2 —á–∞—Å—Ç–∏ (–≤–µ—Ä—Ö/–Ω–∏–∑)
            top_part = image.crop((0, 0, width, height // 2))
            bottom_part = image.crop((0, height // 2, width, height))
            
            for part in [top_part, bottom_part]:
                buffer = io.BytesIO()
                part.save(buffer, format='PNG', quality=95)
                parts.append(buffer.getvalue())
                
        elif split_mode == "grid":
            # –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ 4 —á–∞—Å—Ç–∏ (—Å–µ—Ç–∫–∞ 2x2)
            half_width, half_height = width // 2, height // 2
            
            grid_parts = [
                image.crop((0, 0, half_width, half_height)),  # top-left
                image.crop((half_width, 0, width, half_height)),  # top-right
                image.crop((0, half_height, half_width, height)),  # bottom-left
                image.crop((half_width, half_height, width, height))  # bottom-right
            ]
            
            for part in grid_parts:
                buffer = io.BytesIO()
                part.save(buffer, format='PNG', quality=95)
                parts.append(buffer.getvalue())
        
        return parts
        
    except Exception as e:
        # –í —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
        return [image_data]


class DirectVisionAPI:
    """–ü—Ä—è–º–æ–π –∫–ª–∏–µ–Ω—Ç –¥–ª—è GPT-4 Vision API (–ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω–æ —Ä–∞–±–æ—á–∏–π)."""
    
    def __init__(self, images_dir: Optional[Path] = None):
        load_dotenv()
        self.client = openai.OpenAI()
        self.logger = get_logger(__name__)
        self.images_dir = images_dir
        
        # –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫–∏ –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω–∞ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è
        if self.images_dir:
            self.images_dir.mkdir(parents=True, exist_ok=True)
            (self.images_dir / "original").mkdir(exist_ok=True)
            (self.images_dir / "enhanced").mkdir(exist_ok=True)
            self.logger.info(f"Images will be saved to: {self.images_dir}")
    
    def _save_image_to_disk(self, image_data: bytes, filename: str, subfolder: str = "") -> Optional[Path]:
        """
        –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –Ω–∞ –¥–∏—Å–∫ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏ –∏ –∞–Ω–∞–ª–∏–∑–∞.
        
        Args:
            image_data: –î–∞–Ω–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            filename: –ò–º—è —Ñ–∞–π–ª–∞
            subfolder: –ü–æ–¥–ø–∞–ø–∫–∞ (original/enhanced)
            
        Returns:
            –ü—É—Ç—å –∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–º—É —Ñ–∞–π–ª—É –∏–ª–∏ None –µ—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å
        """
        if not self.images_dir:
            return None
            
        try:
            save_path = self.images_dir / subfolder / filename
            with open(save_path, 'wb') as f:
                f.write(image_data)
            
            self.logger.debug(f"Image saved: {save_path} ({len(image_data)} bytes)")
            return save_path
            
        except Exception as e:
            self.logger.warning(f"Failed to save image {filename}: {e}")
            return None
    
    def _enhance_image_for_ocr(self, image_data: bytes) -> bytes:
        """
        –£–ª—É—á—à–∞–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –¥–ª—è –ª—É—á—à–µ–≥–æ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —Ñ–æ—Ä–º—É–ª –∏ —Ç–µ–∫—Å—Ç–∞.
        
        Args:
            image_data: –ò—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            
        Returns:
            –£–ª—É—á—à–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        """
        try:
            # –û—Ç–∫—Ä—ã–≤–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            image = Image.open(io.BytesIO(image_data))
            
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ RGB –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
            if image.mode != 'RGB':
                image = image.convert('RGB')
            
            # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º —Ä–∞–∑—Ä–µ—à–µ–Ω–∏–µ –¥–ª—è –ª—É—á—à–µ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞
            # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –≤ 1.5 —Ä–∞–∑–∞ –¥–ª—è –±–∞–ª–∞–Ω—Å–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –∏ —Ä–∞–∑–º–µ—Ä–∞
            original_size = image.size
            new_size = (int(original_size[0] * 1.5), int(original_size[1] * 1.5))
            image = image.resize(new_size, Image.Resampling.LANCZOS)
            
            # –£–ª—É—á—à–∞–µ–º –∫–æ–Ω—Ç—Ä–∞—Å—Ç–Ω–æ—Å—Ç—å –¥–ª—è –ª—É—á—à–µ–≥–æ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —Ñ–æ—Ä–º—É–ª
            enhancer = ImageEnhance.Contrast(image)
            image = enhancer.enhance(1.2)  # –ü–æ–≤—ã—à–∞–µ–º –∫–æ–Ω—Ç—Ä–∞—Å—Ç –Ω–∞ 20%
            
            # –£–ª—É—á—à–∞–µ–º —Ä–µ–∑–∫–æ—Å—Ç—å –¥–ª—è —á–µ—Ç–∫–æ—Å—Ç–∏ –º–µ–ª–∫–∏—Ö –¥–µ—Ç–∞–ª–µ–π
            enhancer = ImageEnhance.Sharpness(image)
            image = enhancer.enhance(1.3)  # –ü–æ–≤—ã—à–∞–µ–º —Ä–µ–∑–∫–æ—Å—Ç—å –Ω–∞ 30%
            
            # –ù–µ–±–æ–ª—å—à–æ–µ —É–≤–µ–ª–∏—á–µ–Ω–∏–µ —è—Ä–∫–æ—Å—Ç–∏ –µ—Å–ª–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Ç–µ–º–Ω–æ–µ
            enhancer = ImageEnhance.Brightness(image)
            image = enhancer.enhance(1.1)  # –ü–æ–≤—ã—à–∞–µ–º —è—Ä–∫–æ—Å—Ç—å –Ω–∞ 10%
            
            # –ü—Ä–∏–º–µ–Ω—è–µ–º –ª–µ–≥–∫–æ–µ —Å–≥–ª–∞–∂–∏–≤–∞–Ω–∏–µ –¥–ª—è —É–º–µ–Ω—å—à–µ–Ω–∏—è —à—É–º–∞
            image = image.filter(ImageFilter.SMOOTH_MORE)
            
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –æ–±—Ä–∞—Ç–Ω–æ –≤ bytes
            output_buffer = io.BytesIO()
            image.save(output_buffer, format='PNG', quality=95, optimize=True)
            enhanced_data = output_buffer.getvalue()
            
            self.logger.debug(f"Image enhanced: {len(image_data)} -> {len(enhanced_data)} bytes, "
                            f"size: {original_size} -> {new_size}")
            
            return enhanced_data
            
        except Exception as e:
            self.logger.warning(f"Image enhancement failed: {e}, using original")
            return image_data
        
    def extract_tasks_from_page(self, image_data: bytes, page_number: int, use_split_analysis: bool = True) -> Dict[str, Any]:
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç –∑–∞–¥–∞—á–∏ —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏—Å–ø–æ–ª—å–∑—É—è –ø—Ä—è–º–æ–π GPT-4 Vision API.
        
        Args:
            image_data: –î–∞–Ω–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            page_number: –ù–æ–º–µ—Ä —Å—Ç—Ä–∞–Ω–∏—Ü—ã  
            use_split_analysis: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –¥–ª—è –ª—É—á—à–µ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
        """
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏—Å—Ö–æ–¥–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
        original_filename = f"page_{page_number:03d}_original.png"
        self._save_image_to_disk(image_data, original_filename, "original")
        
        # –£–ª—É—á—à–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –¥–ª—è –ª—É—á—à–µ–≥–æ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è
        enhanced_image_data = self._enhance_image_for_ocr(image_data)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
        enhanced_filename = f"page_{page_number:03d}_enhanced.png"
        enhanced_path = self._save_image_to_disk(enhanced_image_data, enhanced_filename, "enhanced")
        
        # –õ–æ–≥–∏—Ä—É–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è—Ö
        self.logger.debug(f"Page {page_number} images: original={len(image_data)} bytes, enhanced={len(enhanced_image_data)} bytes")
        if enhanced_path:
            self.logger.info(f"Page {page_number} images saved to disk: {enhanced_path.parent}")
        
        if use_split_analysis:
            return self._analyze_with_split_method(enhanced_image_data, page_number)
        else:
            return self._analyze_whole_image(enhanced_image_data, page_number)
    
    def _analyze_with_split_method(self, enhanced_image_data: bytes, page_number: int) -> Dict[str, Any]:
        """
        –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—É —Å —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ–º –Ω–∞ —á–∞—Å—Ç–∏ –¥–ª—è –ª—É—á—à–µ–≥–æ –ø–æ–∫—Ä—ã—Ç–∏—è.
        """
        self.logger.info(f"Page {page_number}: Using split analysis method")
        
        # –†–∞–∑–¥–µ–ª—è–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –Ω–∞ —á–∞—Å—Ç–∏
        image_parts = split_image_for_analysis(enhanced_image_data, "vertical")
        
        all_tasks = []
        combined_processing_time = 0
        combined_tokens = 0
        all_raw_responses = []
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–∞–∂–¥—É—é —á–∞—Å—Ç—å –æ—Ç–¥–µ–ª—å–Ω–æ
        for part_idx, part_data in enumerate(image_parts):
            part_name = "–ª–µ–≤–∞—è_—á–∞—Å—Ç—å" if part_idx == 0 else "–ø—Ä–∞–≤–∞—è_—á–∞—Å—Ç—å"
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —á–∞—Å—Ç–∏ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
            part_filename = f"page_{page_number:03d}_part_{part_idx+1}_{part_name}.png"
            self._save_image_to_disk(part_data, part_filename, "enhanced")
            
            self.logger.debug(f"Page {page_number}: Analyzing {part_name}")
            
            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —á–∞—Å—Ç—å
            part_result = self._analyze_image_part(part_data, page_number, part_name, part_idx + 1)
            
            if part_result.get('success', False):
                part_tasks = part_result.get('response', {}).get('tasks', [])
                
                # –û–±–Ω–æ–≤–ª—è–µ–º location_on_page –¥–ª—è –∑–∞–¥–∞—á –∏–∑ —ç—Ç–æ–π —á–∞—Å—Ç–∏
                for task in part_tasks:
                    task['location_on_page'] = part_name
                    task['analyzed_as_part'] = True
                    task['part_number'] = part_idx + 1
                
                all_tasks.extend(part_tasks)
                combined_processing_time += part_result.get('processing_time', 0)
                combined_tokens += part_result.get('tokens', 0)
                all_raw_responses.append(f"Part {part_idx+1} ({part_name}): {part_result.get('raw_content', '')}")
                
                self.logger.info(f"Page {page_number} {part_name}: Found {len(part_tasks)} tasks")
            else:
                self.logger.warning(f"Page {page_number} {part_name}: Analysis failed - {part_result.get('error', 'Unknown error')}")
        
        return {
            "success": True,
            "response": {
                "page_number": page_number,
                "tasks": all_tasks
            },
            "processing_time": combined_processing_time,
            "json_valid": True,
            "model": "gpt-4o",
            "tokens": combined_tokens,
            "raw_content": "\n\n".join(all_raw_responses),
            "split_analysis_used": True,
            "parts_analyzed": len(image_parts)
        }
    
    def _analyze_image_part(self, part_data: bytes, page_number: int, part_name: str, part_number: int) -> Dict[str, Any]:
        """
        –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –æ—Ç–¥–µ–ª—å–Ω—É—é —á–∞—Å—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è.
        """
        b64_image = base64.b64encode(part_data).decode('utf-8')
        
        # –£–ø—Ä–æ—â–µ–Ω–Ω—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —á–∞—Å—Ç–∏
        prompt = f"""–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —ç—Ç—É —á–∞—Å—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_number} –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ —É—á–µ–±–Ω–∏–∫–∞ ({part_name}).

üéØ –ó–ê–î–ê–ß–ê: –ù–∞–π–¥–∏ –í–°–ï –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –∑–∞–¥–∞—á–∏, —É–ø—Ä–∞–∂–Ω–µ–Ω–∏—è –∏ —Ñ–æ—Ä–º—É–ª—ã –≤ —ç—Ç–æ–π —á–∞—Å—Ç–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è.

üìù –ß–¢–û –ò–°–ö–ê–¢–¨:
‚Ä¢ –¢–µ–∫—Å—Ç–æ–≤—ã–µ –∑–∞–¥–∞—á–∏ —Å –≤–æ–ø—Ä–æ—Å–∞–º–∏
‚Ä¢ –ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–º–µ—Ä—ã –∏ –≤—ã—Ä–∞–∂–µ–Ω–∏—è  
‚Ä¢ –ö–æ–º–∞–Ω–¥—ã –∏ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏
‚Ä¢ –ì–µ–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–∏–µ –∑–∞–¥–∞—á–∏
‚Ä¢ –§–æ—Ä–º—É–ª—ã –∏ —Å–∏–º–≤–æ–ª—ã

üìä –§–û–†–ú–ê–¢ –û–¢–í–ï–¢–ê (—Å—Ç—Ä–æ–≥–æ JSON):
{{
  "page_number": {page_number},
  "tasks": [
    {{
      "task_number": "1",
      "task_text": "–ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç –∑–∞–¥–∞—á–∏",
      "has_image": true,
      "task_type": "—Ç–µ–∫—Å—Ç–æ–≤–∞—è_–∑–∞–¥–∞—á–∞|–º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–π_–ø—Ä–∏–º–µ—Ä|—Ñ–æ—Ä–º—É–ª–∞|–≥–µ–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–∞—è_–∑–∞–¥–∞—á–∞",
      "location_on_page": "{part_name}"
    }}
  ]
}}

‚ú® –¢–†–ï–ë–û–í–ê–ù–ò–Ø:
- –ò–∑–≤–ª–µ–∫–∞–π –í–°–ï –Ω–∞–π–¥–µ–Ω–Ω—ã–µ –∑–∞–¥–∞—á–∏
- –ü–∏—à–∏ –¢–û–ß–ù–´–ô —Ç–µ–∫—Å—Ç –∫–∞–∫ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏
- –î–ª—è —Ñ–æ—Ä–º—É–ª –∏—Å–ø–æ–ª—å–∑—É–π LaTeX
- –ï—Å–ª–∏ –Ω–æ–º–µ—Ä –Ω–µ—è—Å–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–π "unknown-1", "unknown-2"
- –û—Ç–≤–µ—á–∞–π –¢–û–õ–¨–ö–û JSON"""

        try:
            start_time = time.time()
            
            response = self.client.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {
                        "role": "user", 
                        "content": [
                            {"type": "text", "text": prompt},
                            {"type": "image_url", "image_url": {"url": f"data:image/png;base64,{b64_image}"}}
                        ]
                    }
                ],
                max_tokens=1500,
                temperature=0.1
            )
            
            processing_time = time.time() - start_time
            content = response.choices[0].message.content
            
            # –ü–æ–ø—ã—Ç–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ JSON
            try:
                parsed_data = json.loads(content)
                json_valid = True
            except json.JSONDecodeError:
                # –ü—ã—Ç–∞–µ–º—Å—è –∏–∑–≤–ª–µ—á—å JSON –∏–∑ —Ç–µ–∫—Å—Ç–∞
                import re
                json_match = re.search(r'\{.*\}', content, re.DOTALL)
                if json_match:
                    try:
                        parsed_data = json.loads(json_match.group())
                        json_valid = True
                    except json.JSONDecodeError:
                        parsed_data = self._create_fallback_structure(content, page_number)
                        json_valid = False
                else:
                    parsed_data = self._create_fallback_structure(content, page_number)
                    json_valid = False
            
            return {
                "success": True,
                "response": parsed_data,
                "processing_time": processing_time,
                "json_valid": json_valid,
                "model": response.model,
                "tokens": response.usage.total_tokens if response.usage else 0,
                "raw_content": content
            }
            
        except Exception as e:
            return {
                "success": False,
                "error": str(e),
                "processing_time": 0,
                "page_number": page_number
            }
    
    def _analyze_whole_image(self, enhanced_image_data: bytes, page_number: int) -> Dict[str, Any]:
        """
        –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –≤—Å—ë –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Ü–µ–ª–∏–∫–æ–º (–æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π –º–µ—Ç–æ–¥).
        """
        # –ö–æ–¥–∏—Ä—É–µ–º —É–ª—É—á—à–µ–Ω–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
        b64_image = base64.b64encode(enhanced_image_data).decode('utf-8')
        
        # –£–ª—É—á—à–µ–Ω–Ω—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è –ø–æ–ª–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        prompt = f"""–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –í–°–Æ —ç—Ç—É —Å—Ç—Ä–∞–Ω–∏—Ü—É {page_number} —à–∫–æ–ª—å–Ω–æ–≥–æ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ —É—á–µ–±–Ω–∏–∫–∞.

üéØ –ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û: 
- –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –í–°–Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –ø–æ–ª–Ω–æ—Å—Ç—å—é
- –ï—Å–ª–∏ —Å—Ç—Ä–∞–Ω–∏—Ü–∞ —Ä–∞–∑–¥–µ–ª–µ–Ω–∞ –Ω–∞ –∫–æ–ª–æ–Ω–∫–∏ - –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –í–°–ï –∫–æ–ª–æ–Ω–∫–∏
- –ù–ï –ü–†–û–ü–£–°–ö–ê–ô –Ω–∏—á–µ–≥–æ

üîç –ó–ê–î–ê–ß–ê: –ù–∞–π–¥–∏ –í–°–ï –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –∑–∞–¥–∞—á–∏, —É–ø—Ä–∞–∂–Ω–µ–Ω–∏—è –∏ —Ñ–æ—Ä–º—É–ª—ã –Ω–∞ –í–°–ï–ô —Å—Ç—Ä–∞–Ω–∏—Ü–µ.

üìù –ß–¢–û –ò–°–ö–ê–¢–¨ (–ø–æ –≤—Å–µ–π —Å—Ç—Ä–∞–Ω–∏—Ü–µ):
‚Ä¢ –¢–µ–∫—Å—Ç–æ–≤—ã–µ –∑–∞–¥–∞—á–∏: "–°–∫–æ–ª—å–∫–æ...?", "–ì–¥–µ...?", "–ß—Ç–æ...?", "–ö–∞–∫...?"
‚Ä¢ –ö–æ–º–∞–Ω–¥—ã –∏ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏: "–ü–æ–∫–∞–∂–∏...", "–ù–∞–π–¥–∏...", "–†–µ—à–∏...", "–ü–æ–ª–æ–∂–∏...", "–í—ã—á–∏—Å–ª–∏...", "–û–ø—Ä–µ–¥–µ–ª–∏..."
‚Ä¢ –ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–º–µ—Ä—ã: —á–∏—Å–ª–∞, –≤—ã—Ä–∞–∂–µ–Ω–∏—è (2+3, 5-1, 10+5), —É—Ä–∞–≤–Ω–µ–Ω–∏—è
‚Ä¢ –ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ —Ñ–æ—Ä–º—É–ª—ã –∏ —Å–∏–º–≤–æ–ª—ã: +, -, =, >, <, —Ü–∏—Ñ—Ä—ã, –¥—Ä–æ–±–∏
‚Ä¢ –ì–µ–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–∏–µ –∑–∞–¥–∞—á–∏: —Ñ–∏–≥—É—Ä—ã, –∏–∑–º–µ—Ä–µ–Ω–∏—è, —Å—Ä–∞–≤–Ω–µ–Ω–∏—è —Ä–∞–∑–º–µ—Ä–æ–≤
‚Ä¢ –°—Ä–∞–≤–Ω–µ–Ω–∏—è: "–±–æ–ª—å—à–µ/–º–µ–Ω—å—à–µ", "–¥–ª–∏–Ω–Ω–µ–µ/–∫–æ—Ä–æ—á–µ", "–≤—ã—à–µ/–Ω–∏–∂–µ"
‚Ä¢ –°—á—ë—Ç –æ–±—ä–µ–∫—Ç–æ–≤: "–ø–æ—Å—á–∏—Ç–∞–π...", "—Å–æ—Å—á–∏—Ç–∞–π...", "—Å–∫–æ–ª—å–∫–æ –≤—Å–µ–≥–æ..."
‚Ä¢ –õ–æ–≥–∏—á–µ—Å–∫–∏–µ –∑–∞–¥–∞—á–∏: –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏, –∑–∞–∫–æ–Ω–æ–º–µ—Ä–Ω–æ—Å—Ç–∏
‚Ä¢ –ó–∞–¥–∞—á–∏ —Å –∫–∞—Ä—Ç–∏–Ω–∫–∞–º–∏: –æ–ø–∏—Å–∞–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –¥–ª—è –ø–æ–¥—Å—á–µ—Ç–∞
* –ò —Ç–æ–º—É –ø–æ–¥–æ–±–Ω–æ–µ...


üìä –§–û–†–ú–ê–¢ –û–¢–í–ï–¢–ê (—Å—Ç—Ä–æ–≥–æ JSON):
{{
  "page_number": {page_number},
  "tasks": [
    {{
      "task_number": "1",
      "task_text": "–ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç –∑–∞–¥–∞—á–∏ –∏–ª–∏ —Ñ–æ—Ä–º—É–ª—ã",
      "has_image": true,
      "task_type": "—Ç–µ–∫—Å—Ç–æ–≤–∞—è_–∑–∞–¥–∞—á–∞|–º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–π_–ø—Ä–∏–º–µ—Ä|—Ñ–æ—Ä–º—É–ª–∞|–≥–µ–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–∞—è_–∑–∞–¥–∞—á–∞",
      "location_on_page": "–ª–µ–≤–∞—è_—á–∞—Å—Ç—å|–ø—Ä–∞–≤–∞—è_—á–∞—Å—Ç—å|—Ü–µ–Ω—Ç—Ä|–≤–µ—Ä—Ö|–Ω–∏–∑"
    }},
    {{
      "task_number": "2", 
      "task_text": "—Å–ª–µ–¥—É—é—â–∞—è –∑–∞–¥–∞—á–∞",
      "has_image": false,
      "task_type": "–º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–π_–ø—Ä–∏–º–µ—Ä",
      "location_on_page": "–ø—Ä–∞–≤–∞—è_—á–∞—Å—Ç—å"
    }}
  ]
}}

‚ú® –ö–†–ò–¢–ò–ß–ï–°–ö–ò–ï –¢–†–ï–ë–û–í–ê–ù–ò–Ø: 
- –û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–û –ø—Ä–æ—Å–º–æ—Ç—Ä–∏ –í–°–Æ —Å—Ç—Ä–∞–Ω–∏—Ü—É –æ—Ç –∫—Ä–∞—è –¥–æ –∫—Ä–∞—è
- –ï—Å–ª–∏ –≤–∏–¥–∏—à—å –Ω–µ—Å–∫–æ–ª—å–∫–æ –∫–æ–ª–æ–Ω–æ–∫ - –∏–∑–≤–ª–µ–∫–∏ –∑–∞–¥–∞—á–∏ –∏–∑ –í–°–ï–•
- –í–∫–ª—é—á–∞–π –í–°–ï –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ —ç–ª–µ–º–µ–Ω—Ç—ã, –¥–∞–∂–µ –ø—Ä–æ—Å—Ç–µ–π—à–∏–µ
- –î–ª—è —Ñ–æ—Ä–º—É–ª –∏—Å–ø–æ–ª—å–∑—É–π LaTeX
- –ï—Å–ª–∏ –Ω–æ–º–µ—Ä –∑–∞–¥–∞—á–∏ –Ω–µ –≤–∏–¥–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–π "unknown-1", "unknown-2" –∏ —Ç.–¥.
- –£–∫–∞–∑—ã–≤–∞–π –ø—Ä–∏–º–µ—Ä–Ω–æ–µ —Ä–∞—Å–ø–æ–ª–æ–∂–µ–Ω–∏–µ –∑–∞–¥–∞—á–∏ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ
- –ï—Å–ª–∏ –≤ –∑–∞–¥–∞—á–µ –µ—Å—Ç—å –∫–∞—Ä—Ç–∏–Ω–∫–∞ - —É–∫–∞–∂–∏, —á—Ç–æ –æ–Ω–∞ –µ—Å—Ç—å
- –°–ª–µ–¥–∏ –∑–∞ —Ç–µ–º, —á—Ç–æ–±—ã –∑–∞–¥–∞—á–∞ —Ü–µ–ª–∏–∫–æ–º –±—ã–ª–∞ –≤ –æ–¥–Ω–æ–π JSON-–∑–∞–ø–∏—Å–∏
- –ü–∏—à–∏ –°–¢–†–û–ì–û –¢–û–¢ –¢–ï–ö–°–¢, –∫–æ—Ç–æ—Ä—ã–π –≤–∏–¥–µ–Ω –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏, –ù–ï –¥–æ–±–∞–≤–ª—è–π –Ω–∏–∫–∞–∫–∏—Ö –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤, –ù–ï –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä—É–π
- –û—Ç–≤–µ—á–∞–π –¢–û–õ–¨–ö–û JSON, –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞"""

        try:
            start_time = time.time()
            
            response = self.client.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {
                        "role": "user", 
                        "content": [
                            {"type": "text", "text": prompt},
                            {"type": "image_url", "image_url": {"url": f"data:image/png;base64,{b64_image}"}}
                        ]
                    }
                ],
                max_tokens=2000,
                temperature=0.1
            )
            
            processing_time = time.time() - start_time
            content = response.choices[0].message.content
            
            self.logger.info(f"GPT-4 Vision response for page {page_number}: {len(content)} chars")
            
            # –ü–æ–ø—ã—Ç–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ JSON
            try:
                parsed_data = json.loads(content)
                json_valid = True
                self.logger.debug(f"Valid JSON parsed for page {page_number}")
            except json.JSONDecodeError as e:
                # –ï—Å–ª–∏ –Ω–µ JSON, –ø—ã—Ç–∞–µ–º—Å—è –∏–∑–≤–ª–µ—á—å JSON –∏–∑ —Ç–µ–∫—Å—Ç–∞
                json_valid = False
                self.logger.warning(f"JSON parse failed for page {page_number}: {e}")
                
                # –ò—â–µ–º JSON –±–ª–æ–∫ –≤ –æ—Ç–≤–µ—Ç–µ
                import re
                json_match = re.search(r'\{.*\}', content, re.DOTALL)
                if json_match:
                    try:
                        parsed_data = json.loads(json_match.group())
                        json_valid = True
                        self.logger.info(f"Extracted JSON from text for page {page_number}")
                    except json.JSONDecodeError:
                        parsed_data = self._create_fallback_structure(content, page_number)
                else:
                    parsed_data = self._create_fallback_structure(content, page_number)
            
            return {
                "success": True,
                "response": parsed_data,
                "processing_time": processing_time,
                "json_valid": json_valid,
                "model": response.model,
                "tokens": response.usage.total_tokens if response.usage else 0,
                "raw_content": content  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ–ª–Ω—ã–π –æ—Ç–≤–µ—Ç –±–µ–∑ –æ–±—Ä–µ–∑–∞–Ω–∏—è
            }
            
        except Exception as e:
            self.logger.error(f"API error for page {page_number}: {e}")
            return {
                "success": False,
                "error": str(e),
                "processing_time": 0,
                "page_number": page_number
            }
    
    def _create_fallback_structure(self, content: str, page_number: int) -> Dict[str, Any]:
        """–°–æ–∑–¥–∞—ë—Ç fallback —Å—Ç—Ä—É–∫—Ç—É—Ä—É –∫–æ–≥–¥–∞ JSON –Ω–µ —Ä–∞—Å–ø–∞—Ä—Å–∏–ª—Å—è."""
        
        # –ü—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ –∑–∞–¥–∞—á–∏ –≤ —Ç–µ–∫—Å—Ç–µ
        lines = content.split('\n')
        tasks = []
        
        for i, line in enumerate(lines, 1):
            line = line.strip()
            if len(line) > 10 and any(keyword in line.lower() for keyword in 
                                     ['—Å–∫–æ–ª—å–∫–æ', '–Ω–∞–π–¥–∏', '—Ä–µ—à–∏', '–ø–æ–∫–∞–∂–∏', '–ø–æ–ª–æ–∂–∏', '?', '–∑–∞–¥–∞—á']):
                tasks.append({
                    "task_number": f"extracted-{i}",
                    "task_text": line,
                    "has_image": True
                })
        
        if not tasks:
            # –ï—Å–ª–∏ –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞—à–ª–∏, –±–µ—Ä—ë–º –≤–µ—Å—å –∫–æ–Ω—Ç–µ–Ω—Ç –∫–∞–∫ –æ–¥–Ω—É –∑–∞–¥–∞—á—É
            tasks.append({
                "task_number": "fallback-1",
                "task_text": content[:1000] + "..." if len(content) > 1000 else content,
                "has_image": False
            })
        
        return {
            "page_number": page_number,
            "tasks": tasks
        }


class ParallelProcessingManager:
    """–£–ø—Ä–∞–≤–ª—è–µ—Ç –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π —Å —É—á–µ—Ç–æ–º rate limits OpenAI."""
    
    def __init__(self, max_concurrent_requests: int = 5, requests_per_minute: int = 80):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–µ–Ω–µ–¥–∂–µ—Ä–∞ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏.
        
        Args:
            max_concurrent_requests: –ú–∞–∫—Å–∏–º—É–º –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
            requests_per_minute: –ú–∞–∫—Å–∏–º—É–º –∑–∞–ø—Ä–æ—Å–æ–≤ –≤ –º–∏–Ω—É—Ç—É (–æ—Å—Ç–∞–≤–ª—è–µ–º –∑–∞–ø–∞—Å –æ—Ç –ª–∏–º–∏—Ç–∞ 100)
        """
        self.max_concurrent_requests = max_concurrent_requests
        self.requests_per_minute = requests_per_minute
        self.request_times = []
        self.logger = get_logger(__name__)
        
    async def _wait_for_rate_limit(self):
        """–û–∂–∏–¥–∞–µ—Ç –µ—Å–ª–∏ –Ω—É–∂–Ω–æ –¥–ª—è —Å–æ–±–ª—é–¥–µ–Ω–∏—è rate limits."""
        current_time = time.time()
        
        # –£–±–∏—Ä–∞–µ–º –∑–∞–ø—Ä–æ—Å—ã —Å—Ç–∞—Ä—à–µ –º–∏–Ω—É—Ç—ã
        self.request_times = [t for t in self.request_times if current_time - t < 60]
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –º–æ–∂–µ–º –ª–∏ –¥–µ–ª–∞—Ç—å –∑–∞–ø—Ä–æ—Å
        if len(self.request_times) >= self.requests_per_minute:
            wait_time = 60 - (current_time - self.request_times[0])
            if wait_time > 0:
                self.logger.info(f"Rate limit: waiting {wait_time:.1f}s")
                await asyncio.sleep(wait_time)
                await self._wait_for_rate_limit()  # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–Ω–æ–≤–∞ –ø–æ—Å–ª–µ –æ–∂–∏–¥–∞–Ω–∏—è
        
        # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –≤—Ä–µ–º—è —Ç–µ–∫—É—â–µ–≥–æ –∑–∞–ø—Ä–æ—Å–∞
        self.request_times.append(current_time)
    
    async def process_page_async(self, vision_api: DirectVisionAPI, image_data: bytes, page_number: int) -> Dict[str, Any]:
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –æ–¥–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã."""
        await self._wait_for_rate_limit()
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º —Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –º–µ—Ç–æ–¥ –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ
        loop = asyncio.get_event_loop()
        with concurrent.futures.ThreadPoolExecutor() as executor:
            result = await loop.run_in_executor(
                executor, 
                vision_api.extract_tasks_from_page,
                image_data,
                page_number
            )
        
        return result
    
    async def process_pages_batch(self, vision_api: DirectVisionAPI, pages_data: List[tuple]) -> List[Dict[str, Any]]:
        """
        –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –ø–∞–∫–µ—Ç —Å—Ç—Ä–∞–Ω–∏—Ü –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ.
        
        Args:
            vision_api: –≠–∫–∑–µ–º–ø–ª—è—Ä DirectVisionAPI
            pages_data: –°–ø–∏—Å–æ–∫ –∫–æ—Ä—Ç–µ–∂–µ–π (image_data, page_number)
            
        Returns:
            –°–ø–∏—Å–æ–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –æ–±—Ä–∞–±–æ—Ç–∫–∏
        """
        semaphore = asyncio.Semaphore(self.max_concurrent_requests)
        
        async def process_with_semaphore(image_data, page_number):
            async with semaphore:
                return await self.process_page_async(vision_api, image_data, page_number)
        
        # –°–æ–∑–¥–∞–µ–º –∑–∞–¥–∞—á–∏ –¥–ª—è –≤—Å–µ—Ö —Å—Ç—Ä–∞–Ω–∏—Ü
        tasks = [
            process_with_semaphore(image_data, page_number)
            for image_data, page_number in pages_data
        ]
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º –≤—Å–µ –∑–∞–¥–∞—á–∏ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∏—Å–∫–ª—é—á–µ–Ω–∏—è
        processed_results = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                page_number = pages_data[i][1]
                self.logger.error(f"Error processing page {page_number}: {result}")
                processed_results.append({
                    "page_number": page_number,
                    "success": False,
                    "error": str(result),
                    "processing_time": 0
                })
            else:
                processed_results.append(result)
        
        return processed_results


class PureVisionFixedExtractor:
    """–ò—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω–∞—è Pure Vision —Å–∏—Å—Ç–µ–º–∞ —Å –ø—Ä—è–º–æ–π –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–µ–π OpenAI."""
    
    def __init__(self, pdf_path: str, images_dir: Optional[Path] = None):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω–æ–≥–æ Pure Vision —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä–∞.
        
        Args:
            pdf_path: –ü—É—Ç—å –∫ PDF —Ñ–∞–π–ª—É
            images_dir: –ü–∞–ø–∫–∞ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        """
        self.logger = get_logger(__name__)
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
        self.pdf_processor = PDFProcessor(pdf_path)
        self.vision_api = DirectVisionAPI(images_dir)
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º PDF
        self.pdf_processor.load_pdf()
        
        if images_dir:
            self.logger.info(f"Pure Vision Fixed extractor initialized: {pdf_path}, images will be saved to: {images_dir}")
        else:
            self.logger.info(f"Pure Vision Fixed extractor initialized: {pdf_path}")
    
    def extract_tasks_from_page(self, page_number: int, use_split_analysis: bool = True) -> Dict[str, Any]:
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç –∑–∞–¥–∞—á–∏ —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏—Å–ø–æ–ª—å–∑—É—è –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π Pure Vision.
        
        Args:
            page_number: –ù–æ–º–µ—Ä —Å—Ç—Ä–∞–Ω–∏—Ü—ã –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ (1-based)
            
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è
        """
        start_time = time.time()
        
        try:
            # –ü–æ–ª—É—á–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            page_image = self.pdf_processor.convert_page_to_image(page_number - 1, save_to_file=False)  # 0-based index
            
            if not page_image:
                return {
                    "page_number": page_number,
                    "tasks": [],
                    "processing_time": time.time() - start_time,
                    "method": "pure_vision_fixed_no_image",
                    "error": f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_number}"
                }
            
            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —á–µ—Ä–µ–∑ –ø—Ä—è–º–æ–π GPT-4 Vision API
            self.logger.debug(f"Analyzing page {page_number} with Direct GPT-4 Vision")
            
            api_result = self.vision_api.extract_tasks_from_page(page_image, page_number, use_split_analysis)
            
            if not api_result.get('success', False):
                return {
                    "page_number": page_number,
                    "tasks": [],
                    "processing_time": time.time() - start_time,
                    "method": "pure_vision_fixed_api_failed",
                    "error": f"Direct Vision API failed: {api_result.get('error', 'Unknown error')}"
                }
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∑–∞–¥–∞—á–∏ –∏–∑ –æ—Ç–≤–µ—Ç–∞ GPT
            parsed_response = api_result.get('response', {})
            tasks_data = parsed_response.get('tasks', [])
            
            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ –Ω–∞—à —Ñ–æ—Ä–º–∞—Ç
            tasks = []
            for i, task_data in enumerate(tasks_data, 1):
                task = {
                    'task_number': task_data.get('task_number', f"fixed-{page_number}-{i}"),
                    'task_text': task_data.get('task_text', ''),
                    'has_image': task_data.get('has_image', False),
                    'task_type': task_data.get('task_type', 'unknown'),
                    'location_on_page': task_data.get('location_on_page', 'unknown'),
                    'extraction_method': 'pure_vision_fixed_enhanced',
                    'vision_api_used': True,
                    'api_model': api_result.get('model', 'gpt-4o'),
                    'processing_confidence': self._calculate_confidence(task_data, api_result),
                    'pure_vision_fixed': True,
                    'enhanced_prompt': True
                }
                tasks.append(task)
            
            processing_time = time.time() - start_time
            
            self.logger.debug(f"Pure Vision Fixed extracted {len(tasks)} tasks from page {page_number}")
            
            return {
                "page_number": page_number,
                "tasks": tasks,
                "processing_time": processing_time,
                "method": "pure_vision_fixed_success",
                "api_model": api_result.get('model', 'gpt-4o'),
                "api_tokens": api_result.get('tokens', 0),
                "api_time": api_result.get('processing_time', 0),
                "json_valid": api_result.get('json_valid', False),
                "raw_response": api_result.get('raw_content', '')
            }
            
        except Exception as e:
            self.logger.error(f"Error extracting tasks from page {page_number}: {e}")
            return {
                "page_number": page_number,
                "tasks": [],
                "processing_time": time.time() - start_time,
                "method": "pure_vision_fixed_error",
                "error": str(e)
            }
    
    def _calculate_confidence(self, task_data: Dict[str, Any], api_result: Dict[str, Any]) -> float:
        """–í—ã—á–∏—Å–ª—è–µ—Ç confidence score –¥–ª—è Pure Vision Fixed."""
        
        confidence = 0.8  # –ë–∞–∑–æ–≤—ã–π confidence –¥–ª—è Direct API
        
        # –ë–æ–Ω—É—Å –∑–∞ —É—Å–ø–µ—à–Ω—ã–π JSON –ø–∞—Ä—Å–∏–Ω–≥
        if api_result.get('json_valid', False):
            confidence += 0.1
        
        # –ë–æ–Ω—É—Å –∑–∞ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
        task_text = task_data.get('task_text', '').lower()
        math_keywords = ['—Å–∫–æ–ª—å–∫–æ', '–Ω–∞–π–¥–∏', '—Ä–µ—à–∏', '–ø–æ–∫–∞–∂–∏', '–ø–æ–ª–æ–∂–∏', '–ø–æ—Å—á–∏—Ç–∞–π', '?', '+', '-', '=']
        keyword_count = sum(1 for keyword in math_keywords if keyword in task_text)
        confidence += min(keyword_count * 0.02, 0.1)
        
        return min(confidence, 1.0)


class PureVisionFixedStorage:
    """–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–º —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –¥–ª—è Pure Vision Fixed."""
    
    def __init__(self, storage_dir: Path):
        self.storage_dir = storage_dir
        self.storage_dir.mkdir(parents=True, exist_ok=True)
        
    def save_page_result(self, page_number: int, page_data: Dict[str, Any]) -> None:
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç Pure Vision Fixed –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã."""
        file_path = self.storage_dir / f"pure_vision_fixed_page_{page_number:03d}.json"
        
        # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ Pure Vision Fixed
        page_data.update({
            'processed_at': datetime.now().isoformat(),
            'page_number': page_number,
            'storage_version': '5.0-pure-vision-fixed',
            'architecture': 'pure_gpt4_vision_direct_api'
        })
        
        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(page_data, f, ensure_ascii=False, indent=2)
    
    def load_page_result(self, page_number: int) -> Optional[Dict[str, Any]]:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç Pure Vision Fixed –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã."""
        file_path = self.storage_dir / f"pure_vision_fixed_page_{page_number:03d}.json"
        
        if not file_path.exists():
            return None
            
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ Pure Vision Fixed —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_number}: {e}")
            return None
    
    def get_processed_pages(self) -> List[int]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –Ω–æ–º–µ—Ä–æ–≤ —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü."""
        processed = []
        
        for file_path in sorted(self.storage_dir.glob("pure_vision_fixed_page_*.json")):
            try:
                page_num = int(file_path.stem.split('_')[4])
                processed.append(page_num)
            except (ValueError, IndexError):
                continue
                
        return sorted(processed)
    
    def load_all_results(self) -> List[Dict[str, Any]]:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –≤—Å–µ —Å–æ—Ö—Ä–∞–Ω—ë–Ω–Ω—ã–µ Pure Vision Fixed —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã."""
        results = []
        
        for page_num in self.get_processed_pages():
            page_data = self.load_page_result(page_num)
            if page_data:
                results.append(page_data)
        
        return results
    
    def clear_storage(self) -> None:
        """–û—á–∏—â–∞–µ—Ç –≤—Å–µ –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ —Ñ–∞–π–ª—ã Pure Vision Fixed."""
        for file_path in self.storage_dir.glob("pure_vision_fixed_page_*.json"):
            file_path.unlink()


async def process_pages_parallel(extractor, parallel_manager, storage, start_page, end_page, 
                                processed_pages, force, verbose, batch_size):
    """
    –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—ã –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ –ø–∞–∫–µ—Ç–∞–º–∏.
    
    Returns:
        Tuple: (processed_count, skipped_count, error_count, total_tasks, total_tokens, total_api_time)
    """
    processed_count = 0
    skipped_count = 0
    error_count = 0
    total_tasks = 0
    total_tokens = 0
    total_api_time = 0
    
    # –°–æ–±–∏—Ä–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—ã –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
    pages_to_process = []
    for page_num in range(start_page, end_page + 1):
        if not force and page_num in processed_pages:
            skipped_count += 1
            if verbose:
                print(f"‚è≠Ô∏è  –°—Ç—Ä–∞–Ω–∏—Ü–∞ {page_num}: –ø—Ä–æ–ø—É—â–µ–Ω–∞ (—É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–∞)")
            continue
        pages_to_process.append(page_num)
    
    print(f"üì¶ –ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–æ –∫ –æ–±—Ä–∞–±–æ—Ç–∫–µ: {len(pages_to_process)} —Å—Ç—Ä–∞–Ω–∏—Ü")
    
    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –ø–∞–∫–µ—Ç–∞–º–∏
    for i in range(0, len(pages_to_process), batch_size):
        batch_pages = pages_to_process[i:i + batch_size]
        batch_start = i + 1
        batch_end = min(i + batch_size, len(pages_to_process))
        
        print(f"\nüîÑ –ü–∞–∫–µ—Ç {batch_start}-{batch_end} –∏–∑ {len(pages_to_process)} —Å—Ç—Ä–∞–Ω–∏—Ü...")
        
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –¥–ª—è –ø–∞–∫–µ—Ç–∞
        batch_data = []
        for page_num in batch_pages:
            try:
                # –ü–æ–ª—É—á–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
                page_image = extractor.pdf_processor.convert_page_to_image(page_num - 1, save_to_file=False)
                if page_image:
                    batch_data.append((page_image, page_num))
                else:
                    error_count += 1
                    print(f"     ‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_num}")
            except Exception as e:
                error_count += 1
                print(f"     ‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_num}: {e}")
        
        if not batch_data:
            print("     ‚ö†Ô∏è  –ù–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤ –ø–∞–∫–µ—Ç–µ")
            continue
        
        # –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –ø–∞–∫–µ—Ç–∞
        batch_start_time = time.time()
        try:
            batch_results = await parallel_manager.process_pages_batch(
                extractor.vision_api, batch_data
            )
            batch_time = time.time() - batch_start_time
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–∞–∫–µ—Ç–∞
            for result in batch_results:
                page_num = result.get('page_number', 0)
                
                if not result.get('success', False):
                    error_count += 1
                    print(f"     ‚ùå –û—à–∏–±–∫–∞ API –¥–ª—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_num}: {result.get('error', 'Unknown')}")
                    continue
                
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –∑–∞–¥–∞—á–∏ –∏–∑ –æ—Ç–≤–µ—Ç–∞ GPT
                parsed_response = result.get('response', {})
                tasks_data = parsed_response.get('tasks', [])
                
                # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ –Ω–∞—à —Ñ–æ—Ä–º–∞—Ç
                tasks = []
                for j, task_data in enumerate(tasks_data, 1):
                    task = {
                        'task_number': task_data.get('task_number', f"parallel-{page_num}-{j}"),
                        'task_text': task_data.get('task_text', ''),
                        'has_image': task_data.get('has_image', False),
                        'task_type': task_data.get('task_type', 'unknown'),
                        'location_on_page': task_data.get('location_on_page', 'unknown'),
                        'extraction_method': 'pure_vision_fixed_parallel',
                        'vision_api_used': True,
                        'api_model': result.get('model', 'gpt-4o'),
                        'processing_confidence': extractor._calculate_confidence(task_data, result),
                        'pure_vision_fixed': True,
                        'enhanced_prompt': True,
                        'parallel_processed': True
                    }
                    tasks.append(task)
                
                # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
                api_time = result.get('processing_time', 0)
                api_tokens = result.get('tokens', 0)
                json_valid = result.get('json_valid', False)
                
                page_result = {
                    'page_number': page_num,
                    'tasks': tasks,
                    'processing_time': api_time,
                    'method': 'pure_vision_fixed_parallel_success',
                    'api_model': result.get('model', 'gpt-4o'),
                    'api_tokens': api_tokens,
                    'api_time': api_time,
                    'json_valid': json_valid,
                    'task_count': len(tasks),
                    'raw_response': result.get('raw_content', ''),
                    'batch_processed': True
                }
                
                storage.save_page_result(page_num, page_result)
                
                # –û–±–Ω–æ–≤–ª—è–µ–º —Å—á–µ—Ç—á–∏–∫–∏
                processed_count += 1
                total_tasks += len(tasks)
                total_tokens += api_tokens
                total_api_time += api_time
                
                if verbose:
                    confidence_avg = sum(t.get('processing_confidence', 0) for t in tasks) / len(tasks) if tasks else 0
                    print(f"     ‚úÖ –°—Ç—Ä–∞–Ω–∏—Ü–∞ {page_num}: {len(tasks)} –∑–∞–¥–∞—á, JSON={'‚úÖ' if json_valid else '‚ùå'}, conf={confidence_avg:.2f}")
            
            print(f"     üîÑ –ü–∞–∫–µ—Ç –æ–±—Ä–∞–±–æ—Ç–∞–Ω –∑–∞ {batch_time:.1f}s: {len(batch_results)} —Å—Ç—Ä–∞–Ω–∏—Ü")
            
        except Exception as e:
            print(f"     ‚ùå –û—à–∏–±–∫–∞ –ø–∞–∫–µ—Ç–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏: {e}")
            error_count += len(batch_data)
    
    return processed_count, skipped_count, error_count, total_tasks, total_tokens, total_api_time


@click.command()
@click.argument('pdf_file', type=click.Path(exists=True))
@click.argument('output_csv', type=click.Path())
@click.option('--force', is_flag=True, help='–ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –ø–µ—Ä–µ–æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –≤—Å–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã')
@click.option('--start-page', type=int, default=1, help='–ù–∞—á–∞–ª—å–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏')
@click.option('--end-page', type=int, default=None, help='–ö–æ–Ω–µ—á–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏')
@click.option('--production', is_flag=True, help='Production —Ä–µ–∂–∏–º –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è')
@click.option('--verbose', is_flag=True, help='–ü–æ–¥—Ä–æ–±–Ω—ã–π –≤—ã–≤–æ–¥')
@click.option('--parallel', is_flag=True, help='üß™ –≠–ö–°–ü–ï–†–ò–ú–ï–ù–¢–ê–õ–¨–ù–û: –í–∫–ª—é—á–∏—Ç—å –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É (–º–æ–∂–µ—Ç –±—ã—Ç—å –Ω–µ—Å—Ç–∞–±–∏–ª—å–Ω–æ)')
@click.option('--max-concurrent', type=int, default=3, help='–ú–∞–∫—Å–∏–º—É–º –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ –¥–ª—è --parallel (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 3, —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –Ω–µ –±–æ–ª–µ–µ 5)')
@click.option('--batch-size', type=int, default=5, help='–†–∞–∑–º–µ—Ä –ø–∞–∫–µ—Ç–∞ –¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 5)')
@click.option('--split-analysis', is_flag=True, default=True, help='üéØ –†–ï–ö–û–ú–ï–ù–î–£–ï–¢–°–Ø: –†–∞–∑–¥–µ–ª—è—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –Ω–∞ —á–∞—Å—Ç–∏ –¥–ª—è –ª—É—á—à–µ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –º–Ω–æ–≥–æ–∫–æ–ª–æ–Ω–æ—á–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: –≤–∫–ª—é—á–µ–Ω–æ)')
@click.option('--no-split', is_flag=True, help='–û—Ç–∫–ª—é—á–∏—Ç—å —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π (–∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å—Ç–∞—Ä—ã–π –º–µ—Ç–æ–¥ –∞–Ω–∞–ª–∏–∑–∞ —Ü–µ–ª–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã)')
def process_textbook_pure_vision_fixed(pdf_file, output_csv, force, start_page, end_page, production, verbose, parallel, max_concurrent, batch_size, split_analysis, no_split):
    """
    OCR-OCD Pure Vision Fixed: –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω–∞—è –ø—Ä—è–º–∞—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è GPT-4 Vision.
    
    –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ —Å —Ä–∞–±–æ—á–∏–º Direct OpenAI API:
    - PDF ‚Üí –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Å—Ç—Ä–∞–Ω–∏—Ü ‚Üí –ø—Ä—è–º–æ–π GPT-4 Vision ‚Üí CSV
    """
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ª–∏ split analysis
    use_split_analysis = split_analysis and not no_split
    
    print("üöÄüîß OCR-OCD Pure Vision Fixed: Enhanced Direct OpenAI API")
    print("=" * 65)
    print(f"üìñ PDF —Ñ–∞–π–ª: {pdf_file}")
    print(f"üìä –í—ã–≤–æ–¥: {output_csv}")
    print(f"üîÑ Force —Ä–µ–∂–∏–º: {'‚úÖ' if force else '‚ùå'}")
    if parallel:
        print(f"‚ö° –†–µ–∂–∏–º –æ–±—Ä–∞–±–æ—Ç–∫–∏: üß™ –ü–ê–†–ê–õ–õ–ï–õ–¨–ù–´–ô (—ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–π)")
        print(f"üîÑ –û–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤: {max_concurrent}")
        print(f"üì¶ –†–∞–∑–º–µ—Ä –ø–∞–∫–µ—Ç–∞: {batch_size}")
        print(f"‚ö†Ô∏è  –í–ù–ò–ú–ê–ù–ò–ï: –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–π —Ä–µ–∂–∏–º –º–æ–∂–µ—Ç –±—ã—Ç—å –Ω–µ—Å—Ç–∞–±–∏–ª–µ–Ω!")
    else:
        print(f"‚ö° –†–µ–∂–∏–º –æ–±—Ä–∞–±–æ—Ç–∫–∏: üìå –ü–û–°–õ–ï–î–û–í–ê–¢–ï–õ–¨–ù–´–ô (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è)")
        print(f"üí° –î–ª—è —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω–æ–≥–æ –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º–∞ –¥–æ–±–∞–≤—å—Ç–µ --parallel")
    print(f"üéØ –ê–Ω–∞–ª–∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {'üîÑ SPLIT-—Ä–µ–∂–∏–º (—á–∞—Å—Ç–∏)' if use_split_analysis else 'üìÑ –¶–ï–õ–ê–Ø —Å—Ç—Ä–∞–Ω–∏—Ü–∞'}")
    if use_split_analysis:
        print(f"   ‚ú® –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ —á–∞—Å—Ç–∏ –¥–ª—è –ª—É—á—à–µ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –º–Ω–æ–≥–æ–∫–æ–ª–æ–Ω–æ—á–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü")
    print(f"üéØ –£–ª—É—á—à–µ–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥: –ø—Ä—è–º–æ–π OpenAI API + –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π")
    print(f"‚ú® –ü–æ–ª–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Å—Ç—Ä–∞–Ω–∏—Ü + —Ñ–æ—Ä–º—É–ª—ã")
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
    if production:
        setup_production_logger()
    else:
        setup_development_logger()
    
    logger = get_logger(__name__)
    
    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–π –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä –¥–ª—è –≤—Ö–æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞
    print(f"üîç –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —É–Ω–∏–∫–∞–ª—å–Ω–æ–≥–æ –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞ —Ñ–∞–π–ª–∞...")
    file_identifier = generate_file_identifier(pdf_file)
    print(f"üìÅ –ò–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä —Ñ–∞–π–ª–∞: {file_identifier}")
    
    # –°–æ–∑–¥–∞—ë–º –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
    paths = {
        'output': Path("output"),
        'temp': Path("temp"), 
        'logs': Path("logs"),
        'storage': Path(f"temp/processed_pages_pure_vision_fixed_{file_identifier}"),
        'images': Path(f"temp/images_pure_vision_fixed_{file_identifier}")
    }
    
    for path in paths.values():
        path.mkdir(parents=True, exist_ok=True)
    
    print(f"üìÇ –ü—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã: {paths['storage']}")
    print(f"üñºÔ∏è  –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Å–æ—Ö—Ä–∞–Ω—è—é—Ç—Å—è –≤: {paths['images']}")
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Pure Vision Fixed —Ö—Ä–∞–Ω–∏–ª–∏—â–∞
    storage = PureVisionFixedStorage(paths['storage'])
    
    if force:
        print("üóëÔ∏è  Force —Ä–µ–∂–∏–º: –æ—á–∏—Å—Ç–∫–∞ –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤...")
        storage.clear_storage()
        # –¢–∞–∫–∂–µ –æ—á–∏—â–∞–µ–º –ø–∞–ø–∫—É —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏
        if paths['images'].exists():
            import shutil
            shutil.rmtree(paths['images'])
            paths['images'].mkdir(parents=True, exist_ok=True)
    
    start_time = datetime.now()
    
    try:
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Pure Vision Fixed —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä–∞
        print(f"üîß –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Pure Vision Fixed —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä–∞...")
        extractor = PureVisionFixedExtractor(pdf_file, images_dir=paths['images'])
        
        # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ PDF
        total_pages = extractor.pdf_processor.get_page_count()
        
        if end_page is None:
            end_page = total_pages
        else:
            end_page = min(end_page, total_pages)
        
        print(f"‚úÖ Pure Vision Fixed —Å–∏—Å—Ç–µ–º–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞")
        print(f"üìö PDF: {total_pages} —Å—Ç—Ä–∞–Ω–∏—Ü")
        print(f"üéØ –û–±—Ä–∞–±–æ—Ç–∫–∞: —Å—Ç—Ä–∞–Ω–∏—Ü—ã {start_page}-{end_page}")
        print(f"üîß –ü—Ä—è–º–æ–π OpenAI API: –≥–æ—Ç–æ–≤ –∫ —Ä–∞–±–æ—Ç–µ")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        processed_pages = set(storage.get_processed_pages())
        
        if processed_pages and not force:
            print(f"üìã –ù–∞–π–¥–µ–Ω–æ {len(processed_pages)} —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü")
            print(f"üìä –ü–æ—Å–ª–µ–¥–Ω—è—è: {max(processed_pages) if processed_pages else 'none'}")
        
        # –°—á—ë—Ç—á–∏–∫–∏
        processed_count = 0
        skipped_count = 0 
        error_count = 0
        total_tasks = 0
        total_tokens = 0
        total_api_time = 0
        
        print(f"\nüöÄ –ù–∞—á–∏–Ω–∞–µ–º Pure Vision Fixed –æ–±—Ä–∞–±–æ—Ç–∫—É...")
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–µ–Ω–µ–¥–∂–µ—Ä–∞ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
        if parallel:
            parallel_manager = ParallelProcessingManager(
                max_concurrent_requests=max_concurrent,
                requests_per_minute=80  # –û—Å—Ç–∞–≤–ª—è–µ–º –∑–∞–ø–∞—Å –æ—Ç –ª–∏–º–∏—Ç–∞ OpenAI
            )
            print(f"‚ö° –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –≤–∫–ª—é—á–µ–Ω–∞: {max_concurrent} –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤")
            
            # –ó–∞–ø—É—Å–∫–∞–µ–º –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            try:
                processed_count, skipped_count, error_count, total_tasks, total_tokens, total_api_time = loop.run_until_complete(
                    process_pages_parallel(
                        extractor, parallel_manager, storage, 
                        start_page, end_page, processed_pages, force, verbose, batch_size
                    )
                )
            finally:
                loop.close()
        else:
            # –ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ (–æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π –∫–æ–¥)
            for page_num in range(start_page, end_page + 1):
                page_start_time = time.time()
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω—É–∂–Ω–æ –ª–∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É
                if not force and page_num in processed_pages:
                    skipped_count += 1
                    if verbose:
                        print(f"‚è≠Ô∏è  –°—Ç—Ä–∞–Ω–∏—Ü–∞ {page_num}: –ø—Ä–æ–ø—É—â–µ–Ω–∞ (—É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–∞)")
                    continue
                
                try:
                    progress = (page_num - start_page + 1) / (end_page - start_page + 1) * 100
                    print(f"\nüìñ –°—Ç—Ä–∞–Ω–∏—Ü–∞ {page_num}/{end_page} ({progress:.1f}%)")
                    
                    # Pure Vision Fixed –æ–±—Ä–∞–±–æ—Ç–∫–∞
                    if verbose:
                        analysis_mode = "split-–∞–Ω–∞–ª–∏–∑" if use_split_analysis else "—Ü–µ–ª–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞"
                        print(f"  üîß Pure Vision Fixed –∞–Ω–∞–ª–∏–∑ ({analysis_mode})...")
                    
                    result = extractor.extract_tasks_from_page(page_num, use_split_analysis)
                    
                    if 'error' in result:
                        error_count += 1
                        print(f"     ‚ùå –û—à–∏–±–∫–∞: {result['error']}")
                        continue
                    
                    tasks = result['tasks']
                    method = result.get('method', 'unknown')
                    api_time = result.get('api_time', 0)
                    api_tokens = result.get('api_tokens', 0)
                    json_valid = result.get('json_valid', False)
                    
                    # –û–±–Ω–æ–≤–ª—è–µ–º —Å—á—ë—Ç—á–∏–∫–∏
                    total_tokens += api_tokens
                    total_api_time += api_time
                    
                    if verbose:
                        print(f"  ‚úÖ Pure Vision Fixed –∞–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à—ë–Ω")
                        print(f"  üéØ –ú–µ—Ç–æ–¥: {method}")
                        print(f"  üìä JSON –≤–∞–ª–∏–¥–Ω—ã–π: {'‚úÖ' if json_valid else '‚ùå'}")
                        print(f"  üî§ Tokens: {api_tokens}")
                        print(f"  ‚è±Ô∏è API –≤—Ä–µ–º—è: {api_time:.2f}s")
                    
                    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω–æ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
                    page_result = {
                        'page_number': page_num,
                        'tasks': tasks,
                        'processing_time': result['processing_time'],
                        'method': method,
                        'api_model': result.get('api_model', 'gpt-4o'),
                        'api_tokens': api_tokens,
                        'api_time': api_time,
                        'json_valid': json_valid,
                        'task_count': len(tasks),
                        'raw_response': result.get('raw_response', '')
                    }
                    
                    storage.save_page_result(page_num, page_result)
                    
                    processed_count += 1
                    total_tasks += len(tasks)
                    
                    processing_time = time.time() - page_start_time
                    print(f"     ‚úÖ –ó–∞–¥–∞—á: {len(tasks)}, –í—Ä–µ–º—è: {processing_time:.2f}s")
                    
                    if verbose and tasks:
                        for i, task in enumerate(tasks[:3], 1):  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 3
                            confidence = task.get('processing_confidence', 0)
                            print(f"       {i}. [conf:{confidence:.2f}] {task['task_number']}: {task['task_text'][:50]}...")
                    
                    # –ü–∞—É–∑–∞ –¥–ª—è API
                    time.sleep(1.0)  # –†–∞–∑—É–º–Ω–∞—è –ø–∞—É–∑–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
                    
                except Exception as e:
                    logger.error(f"Error processing page {page_num}: {e}")
                    print(f"     ‚ùå –û—à–∏–±–∫–∞ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {page_num}: {e}")
                    error_count += 1
                    continue
        
        # –§–∏–Ω–∞–ª—å–Ω—ã–π —Å–±–æ—Ä —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        print(f"\nüíæ –°–±–æ—Ä —Ñ–∏–Ω–∞–ª—å–Ω—ã—Ö Pure Vision Fixed —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤...")
        all_results = storage.load_all_results()
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –∏—Ç–æ–≥–æ–≤–æ–≥–æ CSV
        if all_results:
            print(f"üìä –°–æ–∑–¥–∞–Ω–∏–µ Pure Vision Fixed CSV —Ñ–∞–π–ª–∞...")
            create_pure_vision_fixed_csv(all_results, output_csv)
        
        # –î–µ—Ç–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        total_time = (datetime.now() - start_time).total_seconds()
        avg_api_time = total_api_time / processed_count if processed_count > 0 else 0
        estimated_cost = total_tokens * 0.00001  # –ü—Ä–∏–º–µ—Ä–Ω–∞—è —Å—Ç–æ–∏–º–æ—Å—Ç—å
        
        print(f"\nüéâ PURE VISION FIXED –û–ë–†–ê–ë–û–¢–ö–ê –ó–ê–í–ï–†–®–ï–ù–ê!")
        print(f"=" * 55)
        print(f"üìä –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
        print(f"   üìö –°—Ç—Ä–∞–Ω–∏—Ü –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {processed_count}")
        print(f"   ‚è≠Ô∏è  –°—Ç—Ä–∞–Ω–∏—Ü –ø—Ä–æ–ø—É—â–µ–Ω–æ: {skipped_count}")
        print(f"   ‚ùå –û—à–∏–±–æ–∫: {error_count}")
        print(f"   üìù –ó–∞–¥–∞—á –∏–∑–≤–ª–µ—á–µ–Ω–æ: {total_tasks}")
        print(f"   üî§ –í—Å–µ–≥–æ tokens: {total_tokens}")
        print(f"   ‚è±Ô∏è  –û–±—â–µ–µ –≤—Ä–µ–º—è: {total_time:.1f}s ({total_time/60:.1f} –º–∏–Ω—É—Ç)")
        print(f"   ‚ö° –°—Ä–µ–¥–Ω–µ–µ API –≤—Ä–µ–º—è: {avg_api_time:.2f}s")
        print(f"   üí∞ –ü—Ä–∏–º–µ—Ä–Ω–∞—è —Å—Ç–æ–∏–º–æ—Å—Ç—å: ${estimated_cost:.4f}")
        if processed_count > 0:
            print(f"   üìà –°—Ä–µ–¥–Ω–µ–µ –∑–∞–¥–∞—á/—Å—Ç—Ä–∞–Ω–∏—Ü–∞: {total_tasks/processed_count:.1f}")
            print(f"   üèÉ –°–∫–æ—Ä–æ—Å—Ç—å: {processed_count/total_time*60:.1f} —Å—Ç—Ä–∞–Ω–∏—Ü/—á–∞—Å")
        
        print(f"\nüìÅ Pure Vision Fixed CSV: {output_csv}")
        print(f"üéØ –ú–µ—Ç–æ–¥: –ü—Ä—è–º–∞—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è OpenAI API")
        print(f"‚ö° –†–µ–∂–∏–º: {'–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–π (—ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–π)' if parallel else '–ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã–π (—Å—Ç–∞–±–∏–ª—å–Ω—ã–π)'}")
        print(f"‚ú® –§–∞–π–ª-—Å–ø–µ—Ü–∏—Ñ–∏—á–Ω–æ–µ –∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏–µ: {file_identifier}")
        
        return True
        
    except Exception as e:
        logger.error(f"Critical Pure Vision Fixed error: {e}")
        print(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ Pure Vision Fixed: {e}")
        return False


def create_pure_vision_fixed_csv(results: List[Dict], output_path: str) -> None:
    """–°–æ–∑–¥–∞—ë—Ç Pure Vision Fixed CSV —Ñ–∞–π–ª –∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤."""
    
    all_tasks = []
    
    for page_result in results:
        page_num = page_result['page_number']
        processing_time = page_result.get('processing_time', 0)
        method = page_result.get('method', 'unknown')
        api_model = page_result.get('api_model', 'gpt-4o')
        api_tokens = page_result.get('api_tokens', 0)
        api_time = page_result.get('api_time', 0)
        json_valid = page_result.get('json_valid', False)
        
        for task_data in page_result.get('tasks', []):
            task_row = {
                'page_number': page_num,
                'task_number': task_data.get('task_number', 'unknown'),
                'task_text': task_data.get('task_text', ''),
                'has_image': task_data.get('has_image', False),
                'task_type': task_data.get('task_type', 'unknown'),
                'location_on_page': task_data.get('location_on_page', 'unknown'),
                'processing_confidence': task_data.get('processing_confidence', 0.0),
                'processing_time': processing_time,
                'api_method': method,
                'api_model': api_model,
                'api_tokens': api_tokens,
                'api_time': api_time,
                'extraction_method': task_data.get('extraction_method', 'pure_vision_fixed_direct'),
                'vision_api_used': task_data.get('vision_api_used', True),
                'pure_vision_fixed': task_data.get('pure_vision_fixed', True),
                'enhanced_prompt': task_data.get('enhanced_prompt', True),
                'parallel_processed': task_data.get('parallel_processed', False),
                'batch_processed': page_result.get('batch_processed', False),
                'json_valid': json_valid,
                'extracted_at': page_result.get('processed_at', ''),
                'word_count': len(task_data.get('task_text', '').split()),
                'system_type': 'pure_gpt4_vision_fixed_enhanced',
                'architecture': 'pdf_direct_openai_api_enhanced'
            }
            all_tasks.append(task_row)
    
    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –Ω–æ–º–µ—Ä—É —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
    all_tasks.sort(key=lambda x: (x['page_number'], -x['processing_confidence']))
    
    # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º CSV
    import csv
    with open(output_path, 'w', newline='', encoding='utf-8') as f:
        if all_tasks:
            fieldnames = list(all_tasks[0].keys())
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            writer.writeheader()
            writer.writerows(all_tasks)
    
    print(f"‚úÖ Pure Vision Fixed CSV —Å–æ–∑–¥–∞–Ω: {len(all_tasks)} –∑–∞–¥–∞—á —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ")
    print(f"üìä –ù–æ–≤—ã–µ –ø–æ–ª—è: task_type, location_on_page, enhanced_prompt, parallel_processed")
    print(f"üîß –£–ª—É—á—à–µ–Ω–∏—è: –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –ø–æ–ª–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Å—Ç—Ä–∞–Ω–∏—Ü, –ø–æ–¥–¥–µ—Ä–∂–∫–∞ –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º–∞")


if __name__ == "__main__":
    process_textbook_pure_vision_fixed() 